{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.5439],\n        [0.6130],\n        [0.6476]], grad_fn=<AddmmBackward>)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "linear_model = nn.Linear(1, 1)\n",
    "model_input = torch.tensor([[2.0], [1.0], [0.5]])\n",
    "linear_model(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(Parameter containing:\n tensor([[-0.0691]], requires_grad=True),\n Parameter containing:\n tensor([0.6821], requires_grad=True))"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "linear_model.weight, linear_model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([11, 1]), torch.Size([11, 1]))"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c).unsqueeze(1) # <1>\n",
    "t_u = torch.tensor(t_u).unsqueeze(1) # <1>\n",
    "\n",
    "t_u.shape, t_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor([ 0,  3,  4,  8,  2,  7,  9,  1, 10]), tensor([5, 6]))"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "linear_model = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(\n",
    "    linear_model.parameters(),\n",
    "    lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[-0.4663]], requires_grad=True),\n Parameter containing:\n tensor([0.8177], requires_grad=True)]"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "list(linear_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, \n",
    "                  t_u_train, t_u_val, t_c_train, t_c_val):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p_train = model(t_u_train)\n",
    "        loss_train = loss_fn(t_p_train, t_c_train)\n",
    "\n",
    "        t_p_val = model(t_u_val)\n",
    "        loss_val = loss_fn(t_p_val, t_c_val)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print('Epoch %d, Training loss %.4f, Validation loss %.4f' % (\n",
    "                    epoch, float(loss_train), float(loss_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1, Training loss 241.3338, Validation loss 40.4524\nEpoch 1000, Training loss 4.0656, Validation loss 1.4108\nEpoch 2000, Training loss 2.9465, Validation loss 2.8939\nEpoch 3000, Training loss 2.9060, Validation loss 3.3888\n\nParameter containing:\ntensor([[5.4986]], requires_grad=True)\nParameter containing:\ntensor([-18.1312], requires_grad=True)\n"
    }
   ],
   "source": [
    "linear_model = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 3000,\n",
    "    optimizer = optimizer,\n",
    "    model = linear_model,\n",
    "    loss_fn = nn.MSELoss(), # 不再使用自己定义的loss\n",
    "    t_u_train = train_t_un,\n",
    "    t_u_val = val_t_un,\n",
    "    t_c_train = train_t_c,\n",
    "    t_c_val = val_t_c)\n",
    "\n",
    "print()\n",
    "print(linear_model.weight)\n",
    "print(linear_model.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Sequential(\n  (0): Linear(in_features=1, out_features=13, bias=True)\n  (1): Tanh()\n  (2): Linear(in_features=13, out_features=1, bias=True)\n)\n[torch.Size([13, 1]), torch.Size([13]), torch.Size([1, 13]), torch.Size([1])]\n0.weight torch.Size([13, 1])\n0.bias torch.Size([13])\n2.weight torch.Size([1, 13])\n2.bias torch.Size([1])\nhidden_linear.weight torch.Size([8, 1])\nhidden_linear.bias torch.Size([8])\noutput_linear.weight torch.Size([1, 8])\noutput_linear.bias torch.Size([1])\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([-0.2541], requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "seq_model = nn.Sequential(\n",
    "            nn.Linear(1, 13),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(13, 1))\n",
    "print(seq_model)\n",
    "\n",
    "print([param.shape for param in seq_model.parameters()])\n",
    "\n",
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "seq_model = nn.Sequential(OrderedDict([\n",
    "    ('hidden_linear', nn.Linear(1, 8)),\n",
    "    ('hidden_activation', nn.Tanh()),\n",
    "    ('output_linear', nn.Linear(8, 1))\n",
    "]))\n",
    "\n",
    "seq_model\n",
    "\n",
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "seq_model.output_linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SubclassModel(\n  (hidden_linear): Linear(in_features=1, out_features=13, bias=True)\n  (hidden_activation): Tanh()\n  (output_linear): Linear(in_features=13, out_features=1, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "class SubclassModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_linear = nn.Linear(1, 13)\n",
    "        self.hidden_activation = nn.Tanh()\n",
    "        self.output_linear = nn.Linear(13, 1)\n",
    "    def forward(self, input):\n",
    "        hidden_t = self.hidden_linear(input)\n",
    "        activated_t = self.hidden_activation(hidden_t)\n",
    "        output_t = self.output_linear(activated_t)\n",
    "        return output_t\n",
    "\n",
    "subclass_model = SubclassModel()\n",
    "subclass_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SubclassFunctionalModel(\n  (hidden_linear): Linear(in_features=1, out_features=14, bias=True)\n  (output_linear): Linear(in_features=14, out_features=1, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "class SubclassFunctionalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_linear = nn.Linear(1, 14)\n",
    "        # 去掉了nn.Tanh()\n",
    "        self.output_linear = nn.Linear(14, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden_t = self.hidden_linear(input)\n",
    "        activated_t = torch.tanh(hidden_t) # nn.Tanh对应的函数\n",
    "        output_t = self.output_linear(activated_t)\n",
    "        return output_t\n",
    "\n",
    "func_model = SubclassFunctionalModel()\n",
    "func_model"
   ]
  }
 ]
}