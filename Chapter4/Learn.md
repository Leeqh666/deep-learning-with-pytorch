# 学习机制
下图简要概述了本章结束时你将要实现的内容。给定输入数据和相应的期望输出（ground truth）以及权重的初始值，模型输入数据（前向传播），然后通过把结果输出与ground truth进行比较来评估误差。为了优化模型的参数，其权重（即单位权重变化引起的误差变化，也即误差相对于参数的梯度）通过使用对复合函数求导的链式法则进行计算（反向传播）。然后，权重的值沿导致误差减小的方向更新。不断重复该过程直到在新数据上的评估误差降至可接受的水平以下。

![](https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter4/4.2.png)

## Pytorch自动求导
这个问题可以通过一个名为autograd的PyTorch模块来解决。PyTorch张量可以记住它们来自什么运算以及其起源的父张量，并且提供相对于输入的导数链。你无需手动对模型求导：不管如何嵌套，只要你给出前向传播表达式，PyTorch都会自动提供该表达式相对于其输入参数的梯度。

**重复调用backward会导致导数在叶节点处累积。因此，如果提前调用了backward，然后再次计算损失并再次调用backward（如在训练循环中一样），那么在每个叶节点上的梯度会被累积（即求和）在前一次迭代计算出的那个叶节点上，导致梯度值不正确。**

**为防止这种情况发生，你需要在每次迭代时将梯度显式清零。可以使用就地方法zero_轻松地做到这一点。**

## PyTorch中的优化器
<code>torch.optim</code>模块，实现了不同的优化策略。
每个优化器构造函数都将参数（通常是将<code>require_grad</code>设置为True的PyTorch张量）作为第一个输入。传递给优化器的所有参数都保留在优化器对象内，以便优化器可以更新其值并访问其<code>grad</code>属性，如图所示。

![](https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter4/4.10.png)

（A）优化器对参数的引用的概念表示，然后（B）根据输入计算损失，（C）对backward的调用会将grad填充到参数内。此时，（D）优化器可以访问grad并计算参数更新。

每个优化器都有两个方法：<code>zero_grad</code>和<code>step</code>。前者将构造时传递给优化器的所有参数的<code>grad</code>属性归零；后者根据特定优化器实施的优化策略更新这些参数的值。

## 在不需要时关闭autograd
无论经过怎样的优化，追踪计算历史都会带来额外的代价，所以你应该在验证过程中避免这些代价，尤其是当模型具有数百万个参数时。

为了解决这个问题，PyTorch允许你通过使用<code>torch.no_grad</code>上下文管理器在不需要时关闭<code>autograd</code>。虽然就小规模问题而言，在速度或内存消耗方面没有任何有意义的优势。